\documentclass[a4paper, 11pt]{article}

\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{ucs}
\usepackage[english]{babel}
\usepackage{lmodern}
\usepackage{mathtools, amsfonts, amsmath}
\usepackage[parfill]{parskip}
\usepackage{fancyhdr}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\pagestyle{fancyplain}
\fancyhead{}
\fancyfoot[L]{}
\fancyfoot[C]{}
\fancyfoot[R]{\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\setlength{\headheight}{13.6pt}

\widowpenalty=1000
\clubpenalty=1000

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}

\title{ 
\normalfont \normalsize 
\textsc{University of Copenhagen} \\ [25pt]
\horrule{0.5pt} \\[0.4cm]
\huge PCSD: Assignment 1 \\
\horrule{2pt} \\[0.5cm]
}

\author{Jens Fredskov (chw752)\\Henrik Bendt (gwk553)\\Ronni Lindsgaard (mxb392)} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}
\maketitle

\section{Fundamental Abstractions} % (fold)
\label{sec:fundamental_abstractions}

We assume a global fixed block size (e.g. $4KB$), and that the Single Address Space is dynamically reserved (i.e. a non-fixed size).

The Single Address Space is a mapping from index to a block on a machine. This means that each machine can have a variable number of blocks. If a machine joins it is simply appended to the list (meaning that we extend the Single Address Space with the number of blocks provided by the new machine). If a machine leaves abruptly, the pointers in the Single Address Space is simply pointing to a null, meaning that a timeout will occur when we try to read or write to the blocks mapped to that machine. If a machine announces its departure the memory blocks of the machine are transferred to other free blocks on other machines. This implies that each entry in the mapping also contains a flag (of 1 bit) determining whether the block is in use and also that there are enough free blocks to transfer the used blocks of the machine leaving. Furthermore the old entry must now point to the entry of the new block.

We do not make any assumptions on the number of machines other than that the combined number of blocks must be less than or equal to the maximum size $N$ we can hold in the memory.

The pseudocode for the API looks as follows:

\begin{algorithmic}
\Function{Read}{addr}
    \State $(F, M, B) \gets table[addr]$
    \If{$F = 1$} 
        \State $(V) \gets$ \Call{Fetch}{$M, B$}
        \State \Return $V$
    \Else
        \State \Return $empty$
    \EndIf
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{Write}{addr, val}
    \State $(F, B, M) \gets table[addr]$
    \State \Call{Set}{$M, B, val$}
    \State $table[addr] \gets (1, M, B)$
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{Fetch}{M, B}
    \State \Call{Send}{$M, (``fetch'', B)$}
    \State \Call{SetTimeout}{1000}
    \State \Call{Receive}{$M, (res, val)$}
    \If{timeout}
        \State \Return $nil$
    \ElsIf{$res = ``error''$}
        \State segfault
    \ElsIf{$res = ``ok''$}
        \State \Return $val$
    \EndIf
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{Set}{M, B, val}
    \State \Call{Send}{$M, (``set'', B, val)$}
    \State \Call{SetTimeout}{1000}
    \State \Call{Receive}{M, res}
    \If{timeout}
        \State segfault
    \ElsIf{$res = ``error''$}
        \State segfault
    \ElsIf{$res = ``ok''$}
        \State \Return
    \EndIf
\EndFunction
\end{algorithmic}

READ does a lookup on the address table and calls fetch on the block and machine, if the block is used, otherwise returns empty. Returns either the value, segfault or nil (in case of timeout).
WRITE does a lookup on the address table and calls set on the block and machine and given value. If set is successful, upddate the lookup table with write flag set to 1. Else return error.
Atomicity is ensured by having a two phased locking mechanism just above the READ and WRITE API, where the queue of requests are handled by some schedule. The WRITE API ensures atomicity by using TCP and timeout.

%TODO: 3+(4)

% section fundamental_abstractions (end)

\section{Techniques for Performance} % (fold)
\label{sec:techniques_for_performance}

An example using concurrency could be as follows: When a single core processor runs multiple threads concurrently, each thread is swapped when waiting for memory (context switch). Here it reduces the latency for all threads, as other threads can do their computations while the previous are waiting for I/O. If however the overhead of swapping threads exceeds the time waiting for memory, the concurrency becomes a latency factor for the thread waiting for memory which is already retrieved. Thus it is not always a positive nor a negative factor but can be a tradeoff.

Batching is used to group requests to avoid the individual overhead and instead get a maybe smaller group overhead. Batching could be used in an I/O bottleneck, where there is an overhead for each message send and where requests can be combined into one message.

Dallying is used to hold back requests in the hope that a later request cancels out the first one, thus removing having to do the first request at all. It can also be used to hold back specific requests in the hope of creating a batch. Dallying could be used in an I/O bottleneck to eliminate writes to the same space, so only the latest is written.

Caching is a fast path optimization because it tries to make a fast path to the most used data, reducing the average latency of accessing data.

% section techniques_for_performance (end)

\section{Questions for Duscussion on Architecture} % (fold)
\label{sec:questions_for_duscussion_on_architecture}

\begin{enumerate}
    \item 
    \item
    \item
    \item
    \item
    \item
\end{enumerate}

% section questions_for_duscussion_on_architecture (end)

\end{document}
